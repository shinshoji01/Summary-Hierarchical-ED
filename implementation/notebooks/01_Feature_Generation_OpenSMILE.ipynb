{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Generation using OpenSMILE\n",
    "---\n",
    "This notebook provides code to extract acoustic features from speech using OpenSMILE. These features are later used to generate a hierarchical emotion distribution (ED).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import opensmile\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tgt\n",
    "\n",
    "def get_words_phones_dir(textgrid):\n",
    "    tier_w = textgrid.get_tier_by_name(\"words\")\n",
    "    text_w = [[interval.end_time, interval.text] for interval in tier_w.intervals]\n",
    "    tier_p = textgrid.get_tier_by_name(\"phones\")\n",
    "    text_p = [[interval.end_time, interval.text] for interval in tier_p.intervals]\n",
    "    word_dir = {}\n",
    "    idx = 0\n",
    "    for i, w in enumerate(text_w):\n",
    "        time, word = w\n",
    "        key = f\"{i}-\"+word\n",
    "        word_dir[key] = []\n",
    "        while True:\n",
    "            time_p, word_p = text_p[idx]\n",
    "            word_dir[key] += [word_p]\n",
    "            idx += 1\n",
    "            if time==time_p:\n",
    "                break\n",
    "    return word_dir, text_w, text_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# Generation of Acoustic Features\n",
    "---\n",
    "\n",
    "In this example, the following parameters are used to generate acoustic features via OpenSMILE:\n",
    "\n",
    "- **`sr`**:  \n",
    "  An integer that specifies the sampling rate of the speech.\n",
    "\n",
    "- **`dataset_dir`**:  \n",
    "  A string that indicates the path to the dataset directory.\n",
    "\n",
    "- **`feature_dir`**:  \n",
    "  A string that specifies the directory where the processed features will be saved. This includes both the OpenSMILE features and the additional features generated later in the process.\n",
    "\n",
    "- **`depth`**:  \n",
    "  An integer that defines the directory depth of each wav file relative to `dataset_dir`. For instance, in ESD, `depth=3` because the file path follows the structure:  \n",
    "  `[speaker]/[emotion]/[data split]/[speaker]_[filename].wav`.\n",
    "\n",
    "- **`wav2gt`**:  \n",
    "  A dictionary that maps each wav file path (key) to its corresponding TextGrid file (value).\n",
    "\n",
    "- **`reset`**:  \n",
    "  A boolean value that indicates whether to reset the feature generation process. If set to `False`, the feature generation will be skipped if the feature path already exists.\n",
    "\n",
    "The code generates two files:\n",
    "\n",
    "- **`alignments`**:  \n",
    "  A dictionary where each key represents a segment type (such as 'utterance', 'words', or 'phonemes') and each value contains the corresponding acoustic features.  \n",
    "  This file will be stored in `[feature_dir]/opensmile/`.\n",
    "\n",
    "- **`word_dir`**:  \n",
    "  A dictionary where each key is a word (with its position included) and the corresponding value is the list of phonemes for that word.  \n",
    "  This file will be stored in `[feature_dir]/words_phones_dir/`.  \n",
    "  For example:  \n",
    "  <img src=\"images/01_worddirs.png\" width=\"500\">\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########## Adjustable Parameters ##########\n",
    "###########################################\n",
    "\n",
    "sr = 16000\n",
    "dataset_dir = \"../Dataset/ESD/\"\n",
    "feature_dir = \"../Features/ESD/\"\n",
    "depth = 3\n",
    "wav2tgt = {path: (\"../Dataset/ESD/textgrid_corpus_directory/\"+\"/\".join(path.split(\"/\")[-(depth+1):])).replace(\".wav\", \".TextGrid\") for path in glob.glob(dataset_dir + \"*/\"*depth + \"*\")}\n",
    "reset = False\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "smile = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.eGeMAPSv02, \n",
    "    feature_level=opensmile.FeatureLevel.Functionals,\n",
    "    sampling_rate=sr,\n",
    ")\n",
    "\n",
    "notexists = []\n",
    "files = glob.glob(dataset_dir+\"*/\"*depth+\"*.wav\")\n",
    "files.sort()\n",
    "for path in tqdm(files):\n",
    "    dn = \"/\".join(path.split(\"/\")[-(depth+1):-1])+\"/\"\n",
    "    bn = os.path.basename(path)[:-4]\n",
    "    feature_path = f\"{feature_dir}opensmile/{dn}{bn}.npy\"\n",
    "    worddir_path = f\"{feature_dir}words_phones_dir/{dn}{bn}.npy\"\n",
    "    if not(reset) and os.path.exists(feature_path) and os.path.exists(worddir_path):\n",
    "        continue\n",
    "    tg_path = wav2tgt[path]\n",
    "    try:\n",
    "        textgrid = tgt.read_textgrid(tg_path)\n",
    "    except FileNotFoundError:\n",
    "        notexists += [tg_path]\n",
    "        continue\n",
    "    word_dir, _, _ = get_words_phones_dir(textgrid)\n",
    "    audio, _ = librosa.load(path, sr=None)\n",
    "    if _!=sr:\n",
    "        audio, _ = librosa.load(path, sr=sr)\n",
    "    alignments = {}\n",
    "\n",
    "    for align in [\"utterance\", \"words\", \"phones\"]:\n",
    "        if align==\"utterance\":\n",
    "            tier = textgrid.get_tier_by_name(\"words\")\n",
    "            start = int(tier.intervals[0].start_time*sr)\n",
    "            end = int(tier.intervals[-1].end_time*sr)\n",
    "            segment = audio[start:end]\n",
    "            collections = np.array(smile.process_signal(segment, sr))\n",
    "            alignments[align] = collections\n",
    "        else:\n",
    "            tier = textgrid.get_tier_by_name(align)\n",
    "            collections = []\n",
    "            for interval in tier.intervals:\n",
    "                segmented_x = audio[int(interval.start_time*sr):int(interval.end_time*sr)]\n",
    "                collections.append(np.array(smile.process_signal(segmented_x, sr))[0])\n",
    "            collections = np.array(collections)\n",
    "            alignments[align] = collections\n",
    "\n",
    "    os.makedirs(os.path.dirname(feature_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(worddir_path), exist_ok=True)\n",
    "    np.save(feature_path, alignments)\n",
    "    np.save(worddir_path, word_dir)\n",
    "    \n",
    "print(\"The following files are not processed due to missing files of TextGrid\")\n",
    "print()\n",
    "print(notexists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
