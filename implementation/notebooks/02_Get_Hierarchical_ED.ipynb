{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install numpy==1.26.4 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip3 install tgt -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip3 install tqdm -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip3 install scikit-learn -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip3 install opensmile -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip3 install librosa==0.10.2.post1 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip3 install pandas==2.2.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip3 install fastdtw==0.3.4 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip3 install pyworld==0.3.4 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip3 install pymcd==0.2.1 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip3 install scipy==1.14.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip3 install pillow==11.1.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "# !pip3 install matplotlib==3.10.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def GetIntensity(scaler, models, feature):\n",
    "    feature = scaler.transform(feature)\n",
    "    length_array = []\n",
    "    for emotion in emos:\n",
    "        bool_list = np.isnan(feature.astype(float)).sum(axis=1).astype(bool)\n",
    "        feature[bool_list] = 0\n",
    "        array = models[emotion].decision_function(feature)\n",
    "        array[bool_list] = np.nan\n",
    "        length_array += [array]\n",
    "    length_array = np.array(length_array)\n",
    "    return length_array\n",
    "\n",
    "def get_words_indices(word_dir):\n",
    "    words_indices = []\n",
    "    for i, w in enumerate(word_dir):\n",
    "        words_indices += [i]*len(word_dir[w])\n",
    "    words_indices = np.array(words_indices)\n",
    "    return words_indices\n",
    "\n",
    "def get_boollist_fastspeech2(words_dir):\n",
    "    bl = [not(key in sil_phones) for key in [e for wd in words_dir.values() for e in wd]]\n",
    "    bl1 = bl.copy()\n",
    "    for idx in range(1, len(bl1)-1):\n",
    "        if not(bl1[idx]):\n",
    "            if bl1[idx-1]:\n",
    "                bl1[idx] = True\n",
    "    bl2 = bl[::-1].copy()\n",
    "    for idx in range(1, len(bl2)-1):\n",
    "        if not(bl2[idx]):\n",
    "            if bl2[idx-1]:\n",
    "                bl2[idx] = True\n",
    "    newbl = np.array(bl1)*np.array(bl2[::-1])\n",
    "    return newbl\n",
    "\n",
    "def GetMinMax_NoOutliers(outputs):\n",
    "    q1, q3 = np.quantile(outputs, [0.25,0.75])\n",
    "    iqr = q3-q1\n",
    "    bool_list = (q1-1.5*iqr<=outputs)*(q3+1.5*iqr>=outputs)\n",
    "    min_ = outputs[bool_list].min()\n",
    "    max_ = outputs[bool_list].max()\n",
    "    return min_, max_, bool_list\n",
    "\n",
    "def normalize_svm(x, min_, max_):\n",
    "    x[x>0] = x[x>0]/max_\n",
    "    x[x<0] = -x[x<0]/min_\n",
    "    return (x+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########## Adjustable Parameters ##########\n",
    "###########################################\n",
    "\n",
    "hed_extractor_path = '../parameters/linearsvm_OpenSMILE.pkl'\n",
    "scaler_path = '../parameters/scaler_OpenSMILE.pkl'\n",
    "dataset_dir = \"../Dataset/ESD/\"\n",
    "feature_dir = \"../Features/ESD/\"\n",
    "depth = 3\n",
    "wav2tgt = {path: (\"../Dataset/ESD/textgrid_corpus_directory/\"+\"/\".join(path.split(\"/\")[-(depth+1):])).replace(\".wav\", \".TextGrid\") for path in glob.glob(dataset_dir + \"*/\"*depth + \"*\")}\n",
    "reset = False\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "emos = [\"Angry\", \"Happy\", \"Sad\", \"Surprise\"]\n",
    "emos.sort()\n",
    "models = pickle.load(open(hed_extractor_path, 'rb'))\n",
    "scaler = pickle.load(open(scaler_path, 'rb'))\n",
    "split_list = [\"utt\", \"words\", \"phones\"]\n",
    "sil_phones = [\"sil\", \"sp\", \"spn\"]\n",
    "\n",
    "nonexists = []\n",
    "files = glob.glob(feature_dir+\"opensmile/\"+\"*/\"*depth+\"*.npy\")\n",
    "files.sort()\n",
    "for path in tqdm(files):\n",
    "    dn = \"/\".join(path.split(\"/\")[-(depth+1):-1])+\"/\"\n",
    "    bn = os.path.basename(path)[:-4]\n",
    "    savepath = f\"{feature_dir}HED/raw/{dn}{bn}.npy\"\n",
    "    if not(reset) and os.path.exists(savepath):\n",
    "        continue\n",
    "\n",
    "    features = np.load(path, allow_pickle=True).item()\n",
    "    try:\n",
    "        words_dir = np.load(path.replace(\"opensmile\", \"words_phones_dir\"), allow_pickle=True).item()\n",
    "    except EOFError:\n",
    "        nonexists += [dn+bn]\n",
    "        continue\n",
    "        \n",
    "    bl = get_boollist_fastspeech2(words_dir)\n",
    "    words_indices = get_words_indices(words_dir)[bl]\n",
    "\n",
    "    iw = GetIntensity(scaler, models, features[\"words\"])\n",
    "    ip = GetIntensity(scaler, models, features[\"phones\"][:len(bl)])[:,bl]\n",
    "    iu = GetIntensity(scaler, models, features[\"utterance\"])\n",
    "\n",
    "    iw = iw[:, words_indices]\n",
    "    iu = np.repeat(iu, ip.shape[1], axis=1)\n",
    "\n",
    "    feature = np.concatenate([ip, iw, iu], axis=0)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(savepath), exist_ok=True)\n",
    "    np.save(savepath, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "########## Adjustable Parameters ##########\n",
    "###########################################\n",
    "\n",
    "training_files = glob.glob(feature_dir+\"HED/raw/*/*/train/*\")\n",
    "reset = True\n",
    "\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "print(\"####################################\")\n",
    "print(\"Compute Min and Max of Training Data\")\n",
    "print(\"####################################\")\n",
    "print()\n",
    "\n",
    "training_files.sort()\n",
    "\n",
    "arrays = []\n",
    "for path in tqdm(training_files):\n",
    "    feature = np.load(path)\n",
    "    arrays += [feature[8:12,0]]\n",
    "    \n",
    "min_list = []\n",
    "max_list = []\n",
    "for e in range(len(emos)):\n",
    "    bl = (1-np.isnan(np.array(arrays)).mean(axis=1).astype(bool)).astype(bool)\n",
    "    min_, max_, _ = GetMinMax_NoOutliers(np.array(arrays)[bl][:, e])\n",
    "    min_list.append(min_)\n",
    "    max_list.append(max_)\n",
    "    print(f\"Emotion: {emos[e]}\")\n",
    "    print(f\"    Minimum Value: {min_}\")\n",
    "    print(f\"    Maximum Value: {max_}\")\n",
    "    \n",
    "print()\n",
    "print(\"##################################\")\n",
    "print(\"Compute Normalized Hierarchical ED\")\n",
    "print(\"##################################\")\n",
    "print()\n",
    "    \n",
    "files = glob.glob(feature_dir+\"HED/raw/\"+\"*/\"*depth+\"*.npy\")\n",
    "files.sort()\n",
    "for path in tqdm(files):\n",
    "    dn = \"/\".join(path.split(\"/\")[-(depth+1):-1])+\"/\"\n",
    "    bn = os.path.basename(path)[:-4]\n",
    "\n",
    "    savepath = f\"{feature_dir}HED/normalized/{dn}{bn}.npy\"\n",
    "    if not(reset) and os.path.exists(savepath):\n",
    "        continue\n",
    "    try:\n",
    "        a = np.load(path)\n",
    "    except(FileNotFoundError, ValueError) as error:\n",
    "        continue\n",
    "\n",
    "    for s, segment in enumerate([\"phones\", \"words\"]):\n",
    "        for e in range(len(emos)):\n",
    "            b = normalize_svm(a[s*len(emos)+e], min_list[e], max_list[e])\n",
    "            b[b<0] = 0\n",
    "            b[b>1] = 1\n",
    "            ser = pd.Series(b)\n",
    "            ser.interpolate(method=\"linear\", limit_direction=\"both\", inplace=True)\n",
    "            a[s*len(emos)+e] = ser.values\n",
    "\n",
    "    for e in range(len(emos)):\n",
    "        iu = normalize_svm(a[8+e], min_list[e], max_list[e])\n",
    "        iu[iu<0] = 0\n",
    "        iu[iu>1] = 1\n",
    "        a[8+e] = iu\n",
    "\n",
    "    a[np.isnan(a)] = 0 # this happens when all features are nan\n",
    "    os.makedirs(os.path.dirname(savepath), exist_ok=True)\n",
    "    np.save(savepath, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
